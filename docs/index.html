<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="OPD: Single-view 3D Openable Part Detection">
    <meta name="author" content="Hanxiao Jiang,
                                 Yongsen Mao,
                                 Manolis Savva,
                                 Angel X. Chang">

    <title>OPD: Single-view 3D Openable Part Detection</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->

    <!-- Loads <model-viewer> for modern browsers: -->
    <script type="module"
            src="https://unpkg.com/@google/model-viewer/dist/model-viewer.js">
    </script>
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>OPD: Single-view 3D Openable Part Detection</h2>
    <!-- <h2>CVPR 2022</h2> -->
    <hr>
    <p class="authors">
        <a href="https://jianghanxiao.github.io/"> Hanxiao Jiang</a>, 
        <a href="https://github.com/SamMaoYS"> Yongsen Mao</a>,
        <a href="https://msavva.github.io/"> Manolis Savva</a>,
        <a href="https://angelxuanchang.github.io/"> Angel X. Chang</a>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://github.com/3dlg-hcvc/OPD">Code</a> <!-- TODO: -->
        <a class="btn btn-primary" href="">Paper</a> <!-- TODO: -->
    </div>
</div>

<div class="container">
    <div class="section">
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/input_output.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </video>
            </div>
        </div>
        <hr>
        <p>
            We address the task of predicting what parts of an object can open and how they move when they do so. The input is a single image of an object, and as output we detect what parts of the object can open, and the motion parameters describ- ing the articulation of each openable part. To tackle this task, we create two datasets of 3D objects: OPDSynth based on existing synthetic objects, and OPDReal based on RGBD reconstructions of real objects. We then design OPDRCNN, a neural architecture that detects openable parts and predicts their motion parameters. Our experiments show that this is a challenging task especially when considering general- ization across object categories, and the limited amount of information in a single image. Our architecture outperforms baselines and prior work especially for RGB image inputs.
        </p>
    </div>

    <div class="section">
        <h2>Video</h2>
		<hr>
		<div class="vcontainer">
			<iframe class="video" src="https://www.youtube.com/embed/P85iCaD0rfc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		</div><!-- TODO: -->
    </div>

    <div class="section">
	    <h2>Overview</h2>
		<hr>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
					<img src="img/data_example.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
            </div>
        </div>
        <hr>
        <p>
            Example articulated objects from the OPDSynth dataset and the OPDReal dataset. The first row is from OPDSynth. Left: different openable part categories (lid, in orange, drawer in green, door in red). Right: Cabinet objects with different kinematic structures and varying numbers of openable parts. The second row is from our OPDReal dataset. Left: reconstructed cabinet and its semantic part segmentation. Right: example reconstructed objects from different categories.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
					<img src="img/network.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
            </div>
        </div>
        <hr>
        <p>
            Illustration of the network structure for our OPDRCNN-C and OPDRCNN-O architectures. We leverage a MaskRCNN backbone to detect openable parts. Additional heads are trained to predict motion parameters for each part.
        </p>
    </div>

    <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href="" 
                   class="list-group-item">
                    <img src="img/paper_thumbnails.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Download</h2>
        <hr>
        <div>
            <div class="download files">
                <p>Below are datasets and pre-trained models for our three repos to download</p>
                <a href="https://github.com/3dlg-hcvc/OPD">OPD Project</a> :&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://aspis.cmpt.sfu.ca/projects/motionnet/opd/dataset/OPD/dataset.tar.gz">OPD Dataset</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://aspis.cmpt.sfu.ca/projects/motionnet/opd/models/OPD/models.tar.gz">OPD Pre-trained Models</a> 
                <br/>
                <a href="https://github.com/3dlg-hcvc/OPDPN">OPDPN Baseline Project</a> :&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://aspis.cmpt.sfu.ca/projects/motionnet/opd/dataset/OPDPN/dataset.tar.gz">OPDPN Dataset</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://aspis.cmpt.sfu.ca/projects/motionnet/opd/models/OPDPN/models.tar.gz">OPDPN Pre-trained Models</a> 
                <br/>
                <a href="https://github.com/3dlg-hcvc/ANCSH-pytorch">ANCSH Baseline Project</a> :&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://aspis.cmpt.sfu.ca/projects/motionnet/opd/dataset/ANCSH/dataset.tar.gz">ANCSH Dataset</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://aspis.cmpt.sfu.ca/projects/motionnet/opd/models/ANCSH/models.tar.gz">ANCSH Pre-trained Models</a> 
            </div>
        </div>
    </div>

    
    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @misc{jiang2022opd,
                title={OPD: Single-view 3D Openable Part Detection}, 
                author={Yue Ruan and Han-Hung Lee and Ke Zhang and Angel X. Chang},
                year={2022},
                primaryClass={cs.CV}
            }
            
        </div>
    </div>

    <div class="section">
        <h2>Acknowledgements</h2>
        <hr>
        <div class="acknowledgements">
            This work was funded in part by a Canada CIFAR AI Chair, a Canada Research Chair and NSERC Discovery Grant, and enabled in part by support provided by <a href="www.westgrid.ca">WestGrid</a>
            and <a href="www.computecanada.ca">Compute Canada</a>. 
            We thank  <a href="https://scholar.google.com/citations?user=boFO-7gAAAAJ&hl=en">Sanjay Haresh</a> for helping with scanning and narration for our video,
            <a href="https://www.linkedin.com/in/yue-ruan-127a9a15b/">Yue Ruan</a> for scanning and data annotation, and <a href="https://supriya-gdptl.github.io/">Supriya Pandhre</a>, 
            <a href="https://www.linkedin.com/in/xiaohao-sun-237537195/">Xiaohao Sun</a>, and Qirui Wu for annotation.
        </div>
    </div>

    <hr>

    <footer>
        <p>Send feedback and questions to <a href="mailto: shawn_jiang@sfu.ca">Hanxiao Jiang</a>. Website template from <a href="https://www.vincentsitzmann.com/metasdf/">MetaSDF</a>. </p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
